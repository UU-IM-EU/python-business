<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="last-modified" content="2020-06-15 16:14:48 -0500">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- meta "search-domain" used for google site search function google_search() -->
    <meta name="search-domain" value="">
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/lesson.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/syntax.css" />
    
    <link rel="shortcut icon" type="image/x-icon" href="/favicon-swc.ico" />
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
    <title>Python for Business: Linear regression</title>
  </head>
  <body>
    <div class="container">
      
<nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      
      

      
      <a class="navbar-brand" href="../">Home</a>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

	
        <li><a href="../conduct/">Code of Conduct</a></li>

        
	
        <li><a href="../setup/">Setup</a></li>

        
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Episodes <span class="caret"></span></a>
          <ul class="dropdown-menu">
            
            <li><a href="../00-intro_notebook/">Introduction to Jupyter Notebooks</a></li>
            
            <li><a href="../01-intro/">Introduction to Python</a></li>
            
            <li><a href="../02-program_behavior/">Controlling Program Behavior</a></li>
            
            <li><a href="../03-pandas/">Working with Tabular Data</a></li>
            
            <li><a href="../04-functions/">Python basics 3 - Functions and Modules</a></li>
            
            <li><a href="../05-merge/">Working with Tabular Data</a></li>
            
            <li><a href="../06-stat_vis/">Statistical Analysis and Visualization</a></li>
            
            <li><a href="../07-errors/">Extra - Errors and Exceptions</a></li>
            
            <li><a href="../08-debugging/">Extra - Debugging</a></li>
            
            <li><a href="../09-data_prep/">Data Preparation techniques</a></li>
            
            <li><a href="../10-linear_regression/">Linear regression</a></li>
            
            <li><a href="../11-crisp/">Introduction to CRISP-DM</a></li>
            
            <li><a href="../12-case_study/">Case Study</a></li>
            
            <li><a href="../13-case_study2/">Case Study 2 - Yellow Cab Chicago Case</a></li>
            
	    <li role="separator" class="divider"></li>
            <li><a href="../aio/">All in one page (Beta)</a></li>
          </ul>
        </li>
	

	
	
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Extras <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../reference/">Reference</a></li>
            
            <li><a href="../about/">About</a></li>
            
            <li><a href="../discuss/">Discussion</a></li>
            
            <li><a href="../extras/figures.html">Figures</a></li>
            
            <li><a href="../guide/">Instructor Notes</a></li>
            
          </ul>
        </li>
	

	
        <li><a href="../LICENSE.html">License</a></li>
	
	<li><a href="/edit/gh-pages/_episodes/10-linear_regression.md">Improve this page <span class="glyphicon glyphicon-pencil" aria-hidden="true"></span></a></li>
	
      </ul>
      <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form>
    </div>
  </div>
</nav>


<div class="row">
  <div class="col-xs-1">
    <h3 class="text-left">
      
      <a href="../09-data_prep/"><span class="glyphicon glyphicon-menu-left" aria-hidden="true"></span><span class="sr-only">previous episode</span></a>
      
    </h3>
  </div>
  <div class="col-xs-10">
    
    <h3 class="maintitle"><a href="../">Python for Business</a></h3>
    
  </div>
  <div class="col-xs-1">
    <h3 class="text-right">
      
      <a href="../11-crisp/"><span class="glyphicon glyphicon-menu-right" aria-hidden="true"></span><span class="sr-only">next episode</span></a>
      
    </h3>
  </div>
</div>

<article>
<div class="row">
  <div class="col-md-1">
  </div>
  <div class="col-md-10">
    <h1 class="maintitle">Linear regression</h1>
  </div>
  <div class="col-md-1">
  </div>
</div>


<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 60 min
      <br/>
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How to construct linear regression in python?</p>
</li>
	
	<li><p>Continuous or Categorical?</p>
</li>
	
	<li><p>How to interpret regression result?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Construct linear regression in python.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="fitting-data-to-a-model">Fitting Data to a Model</h2>

<p>With data points available for certain values, we can build a more general predictive model using linear regression.  What we’ll do is fit a linear response to each variable from a data set; in fancy terms,</p>

<script type="math/tex; mode=display">y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n</script>

<p>for $n$ values.  A linear regression model makes a few key assumptions:</p>

<ol>
  <li>Features are linearly independent of each other.</li>
  <li>Errors in the regression are normally distributed (rather than biased).</li>
  <li>Effects are linear functions of inputs.</li>
</ol>

<p>The model $y$ is an <em>effect</em> or <em>response</em> produced by the <em>features</em>, the <em>predictors</em>, or the <em>values</em> (all meaning more or less the same thing to us).  If only one feature $x_1$ is used, this is a simple linear regression; if more than one feature is used, as above, then this is a multiple linear regression.</p>

<p>Linear regression is actually implemented several times in Python libraries.  We will use the Python library <code class="highlighter-rouge">statsmodels</code> to construct a regression model over a fuel efficiency dataset, which can be loaded from Seaborn with the <code class="highlighter-rouge">load_dataset()</code> function.  We would like to build a regression model to predict an automobile’s fuel efficiency (in mpg or miles per gallon) from vehicle features.</p>

<p>First, we load and clean the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">mpg</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">"mpg"</span><span class="p">)</span>

<span class="c1"># Display first few rows
</span><span class="n">mpg</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="../pic/df_mpg.png" alt="pt" height="150px" /></p>

<p>Load and examine the data set:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mpg</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>

<blockquote class="output">
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;</span><span class="k">class</span> <span class="err">'</span><span class="nc">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span><span class="s">'&gt;</span><span class="err">
</span><span class="s">RangeIndex: 398 entries, 0 to 397</span><span class="err">
</span><span class="s">Data columns (total 9 columns):</span><span class="err">
</span><span class="s">mpg             398 non-null float64</span><span class="err">
</span><span class="s">cylinders       398 non-null int64</span><span class="err">
</span><span class="s">displacement    398 non-null float64</span><span class="err">
</span><span class="s">horsepower      392 non-null float64</span><span class="err">
</span><span class="s">weight          398 non-null int64</span><span class="err">
</span><span class="s">acceleration    398 non-null float64</span><span class="err">
</span><span class="s">model_year      398 non-null int64</span><span class="err">
</span><span class="s">origin          398 non-null object</span><span class="err">
</span><span class="s">name            398 non-null object</span><span class="err">
</span><span class="s">dtypes: float64(4), int64(3), object(2)</span><span class="err">
</span><span class="s">memory usage: 28.1+ KB</span><span class="err">
</span></code></pre></div>  </div>
</blockquote>

<p>The dataset has nine columns, with self-explanatory titles.  There are 398 rows in the dataset.  One column, <code class="highlighter-rouge">horsepower</code>, has 6 missing values.  We focus on linear regression in this lesson so we simply drop the rows with missing values to get a clean dataset.  (This is by no means the best approach since we will lose valuable data points.  One alternative is estimating horsepower by other factors, ie. displacement and acceleration.)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#simply drop missing values
</span><span class="n">mpg</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">mpg</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>

<blockquote class="output">
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;</span><span class="k">class</span> <span class="err">'</span><span class="nc">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span><span class="s">'&gt;</span><span class="err">
</span><span class="s">Int64Index: 392 entries, 0 to 397</span><span class="err">
</span><span class="s">Data columns (total 9 columns):</span><span class="err">
</span><span class="s">mpg             392 non-null float64</span><span class="err">
</span><span class="s">cylinders       392 non-null int64</span><span class="err">
</span><span class="s">displacement    392 non-null float64</span><span class="err">
</span><span class="s">horsepower      392 non-null float64</span><span class="err">
</span><span class="s">weight          392 non-null int64</span><span class="err">
</span><span class="s">acceleration    392 non-null float64</span><span class="err">
</span><span class="s">model_year      392 non-null int64</span><span class="err">
</span><span class="s">origin          392 non-null object</span><span class="err">
</span><span class="s">name            392 non-null object</span><span class="err">
</span><span class="s">dtypes: float64(4), int64(3), object(2)</span><span class="err">
</span><span class="s">memory usage: 30.6+ KB</span><span class="err">
</span></code></pre></div>  </div>
</blockquote>

<h3 id="descriptive-statistics">Descriptive Statistics</h3>

<p>Our next step is to compute the basic descriptive statistics for the features in this data set.  To accomplish this, we can use the <code class="highlighter-rouge">.describe()</code> function of the <code class="highlighter-rouge">DataFrame</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mpg</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../pic/mpg_describe.png" alt="pt" height="250px" /></p>

<h3 id="regression-model">Regression Model</h3>

<p>With a clean dataset and a cursory understanding of the basic statistics, we are ready to build a model to predict fuel efficiency.  The dependent variable (or <em>response</em>) will be <code class="highlighter-rouge">mpg</code>.  We need to pick a set of candidate dependent variables from the other eight columns.</p>

<p>There are two types of variables in the dataset, <em>categorical</em> and <em>continuous</em>:</p>

<ul>
  <li>Categorical Variables:  the value is limited and usually based on a particular finite group.  E.g., <code class="highlighter-rouge">origin</code>.</li>
  <li>Continuous Variables:  numeric variables that have an infinite number of gradations between any two values. E.g., <code class="highlighter-rouge">horsepower</code>.</li>
</ul>

<p>In a regression model, dependent variables must be continuous.  We don’t have a really great way to handle categorical dependent variables except to create dummy variables for them.  Taking <code class="highlighter-rouge">origin</code> in this dataset as an example, there are three unique values:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mpg</span><span class="o">.</span><span class="n">origin</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</code></pre></div></div>

<blockquote>
  <div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array(['usa', 'japan', 'europe'], dtype=object)
</code></pre></div>  </div>
</blockquote>

<p>To create dummy variables for <code class="highlighter-rouge">origin</code>, we can add two new columns to the dataset, say <code class="highlighter-rouge">origin_usa</code> and <code class="highlighter-rouge">origin_japan</code>.</p>

<ul>
  <li>If <code class="highlighter-rouge">origin</code> is <code class="highlighter-rouge">usa</code>, <code class="highlighter-rouge">origin_usa=1</code> and <code class="highlighter-rouge">origin_japan=0</code>;</li>
  <li>If <code class="highlighter-rouge">origin</code> is <code class="highlighter-rouge">japan</code>, <code class="highlighter-rouge">origin_usa=0</code> and <code class="highlighter-rouge">origin_japan=1</code>;</li>
  <li>If <code class="highlighter-rouge">origin</code> is <code class="highlighter-rouge">europe</code>, <code class="highlighter-rouge">origin_usa=0</code> and <code class="highlighter-rouge">origin_japan=0</code>.</li>
</ul>

<p>(Even though there are three distinct values, we only need two dummy columns to cover all the cases.)</p>

<p><code class="highlighter-rouge">statsmodels</code> provides a way to automatically create dummy variables for categorical variables, so we don’t have to do it manually.</p>

<p>For this regression, <code class="highlighter-rouge">mpg</code> will be the dependent variable.  Let’s choose <code class="highlighter-rouge">horsepower</code>, <code class="highlighter-rouge">weight</code> and <code class="highlighter-rouge">origin</code> for the independent variables.  Among these, <code class="highlighter-rouge">origin</code> is a categorical variable while the other two are continuous.  (Criteria for selecting independent variables can become very complicated.)</p>

<p>To construct the regression model, we define a string-formula using column names (column names can’t have whitespaces).  For this dataset and the variables we chose, the formula is defined as:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mpg</span> <span class="o">~</span> <span class="n">horsepower</span> <span class="o">+</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">C</span><span class="p">(</span><span class="n">origin</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">C(origin)</code> indicates that <code class="highlighter-rouge">origin</code> is a categorical variable.</p>

<p>We next construct the model using the string formula and the dataset, then fit the model, and display the summary of the regression result.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="n">smf</span>

<span class="n">formula</span> <span class="o">=</span> <span class="s">'mpg ~ horsepower + weight + C(origin)'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../pic/mpg_regression.png" alt="pt" height="650px" /></p>

<h3 id="interpreting-regression-results">Interpreting Regression Results</h3>

<p>The resulting tables shows the dependent variable, the model, and the method.  OLS stands for <a href="http://setosa.io/ev/ordinary-least-squares-regression/">Ordinary Least Squares</a>, meaning that we’re trying to fit a model that minimizes the square of distance of each point from the model regression line.</p>

<p>This model has an $R^2$ value of 0.719, meaning that 71.9% of the variance in our dependent variable (<code class="highlighter-rouge">mpg</code>) can be explained by this model.</p>

<p>Based on the coefficient values, we can construct the regression equation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mpg</span> <span class="o">=</span> <span class="mf">43.7</span> <span class="o">-</span> <span class="mf">0.0535</span> <span class="n">horsepower</span> <span class="o">-</span> <span class="mf">0.0048</span> <span class="n">weight</span> <span class="o">+</span> <span class="mf">1.7811</span> <span class="n">origin</span><span class="o">.</span><span class="n">japan</span> <span class="o">-</span> <span class="mf">0.9611</span> <span class="n">origin</span><span class="o">.</span><span class="n">usa</span>
</code></pre></div></div>

<p>This reads:</p>

<ol>
  <li>If <code class="highlighter-rouge">horsepower</code> increases by 1, <code class="highlighter-rouge">mpg</code> will decrease by 0.0535.</li>
  <li>If <code class="highlighter-rouge">weight</code> increases by 1 pound, <code class="highlighter-rouge">mpg</code> will decrease by 0.0048.</li>
  <li>If the <code class="highlighter-rouge">origin</code> is Japan, <code class="highlighter-rouge">mpg</code> will increase by 1.7811 (compared to European cars).</li>
  <li>If the <code class="highlighter-rouge">origin</code> is USA, <code class="highlighter-rouge">mpg</code> will decrease by 0.9611 (compared to European cars).</li>
</ol>

<p>For an European car, <code class="highlighter-rouge">origin.japan=0</code> and <code class="highlighter-rouge">origin.usa=0</code>, yielding the equation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mpg</span> <span class="o">=</span> <span class="mf">43.7</span> <span class="o">-</span> <span class="mf">0.0535</span> <span class="n">horsepower</span> <span class="o">-</span> <span class="mf">0.0048</span> <span class="n">weight</span>
</code></pre></div></div>

<p>This means that the impact of <code class="highlighter-rouge">origin</code> has already been incorporated in the $y$-intercept.  This explains why the impact of <code class="highlighter-rouge">origin.japan</code> and <code class="highlighter-rouge">origin.usa</code> is on top of European cars.</p>

<table>
  <tbody>
    <tr>
      <td>So far, so good:  we have a model that can predict automotive gas mileage using <code class="highlighter-rouge">horsepower</code>, <code class="highlighter-rouge">weight</code>, and <code class="highlighter-rouge">origin</code> to a certain extent ($R^2=0.719$).  But looking into the coefficient table further, we can see some statistical information which illuminates what’s going on.  In particular, <em>$t$ scores</em> and <em>$p$ values</em> ($p&gt;</td>
      <td>t</td>
      <td>$) provide support for a hypothesis test.  $</td>
      <td>t</td>
      <td>&gt;2$ or $p&lt;0.05$ indicates that a coefficient is statistically significant (at 95% confidence).  The coefficient of <code class="highlighter-rouge">origin.usa</code> has a $p$ value of 0.134, which means it’s not statistically significant, or at least that we can’t reject the null hypothesis that the coefficient is 0 (at 95% confidence).  This is also indicated by the 95% confidence interval of the coefficient, -2.220 to 0.298, which includes 0.  Based on the $p$-value, we can say that Japanese cars are more efficient than European cars holding horsepower and weight constant, but we can’t say the same thing for American cars at 95% confidence level.</td>
    </tr>
  </tbody>
</table>

<h3 id="is-model_year-categorical-or-continuous">Is <code class="highlighter-rouge">model_year</code> categorical or continuous?</h3>

<p>Fuel efficiency generally improves over time, so it make sense to consider <code class="highlighter-rouge">model_year</code> in the regression model as independent variable. The <code class="highlighter-rouge">model_year</code> column contains numeric values, we can add it as a continuous variable. The formula then looks like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mpg</span> <span class="o">~</span> <span class="n">horsepower</span> <span class="o">+</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">C</span><span class="p">(</span><span class="n">origin</span><span class="p">)</span> <span class="o">+</span> <span class="n">model_year</span>
</code></pre></div></div>

<p>However, this model assumes that fuel efficiency changes linearly over time, which is not obviously true.  It is, in fact, better to add <code class="highlighter-rouge">model_year</code> into the regression model as a categorical feature, leadind to the following formula:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mpg</span> <span class="o">~</span> <span class="n">horsepower</span> <span class="o">+</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">C</span><span class="p">(</span><span class="n">origin</span><span class="p">)</span> <span class="o">+</span> <span class="n">C</span><span class="p">(</span><span class="n">model_year</span><span class="p">)</span>
</code></pre></div></div>

<p>We will compare these two models:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">formula</span> <span class="o">=</span> <span class="s">'mpg ~ horsepower + weight + C(origin) + model_year'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../pic/mpg_regression2.png" alt="pt" height="550px" /></p>

<p>We can see that adding <code class="highlighter-rouge">model_year</code> to the model increases $R^2$ from 0.719 to 0.819.  The coefficient of <code class="highlighter-rouge">model_year</code> is 0.7544, which means <code class="highlighter-rouge">mpg</code> increases by 0.7544 each year.  The coefficient is also statistically significant.  Let’s see what we get when we add <code class="highlighter-rouge">model_year</code> to the regression model as a categorical variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">formula</span> <span class="o">=</span> <span class="s">'mpg ~ horsepower + weight + C(origin) + C(model_year)'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../pic/mpg_regression3.png" alt="pt" height="850px" /></p>

<p>The new model has $R^2 = 0.852$.  Looking into the coefficients of dummy variables of <code class="highlighter-rouge">model_year</code>, we can see that from 1971 to 1976, there’s not much improvement in vehicle <code class="highlighter-rouge">mpg</code>; while from 1977 to 1982, vehicle <code class="highlighter-rouge">mpg</code> has significant improvements every year.  This model gives better $R^2$, revealing insight into fuel economy changes over time.  One drawback of this model is that we can’t predict vehicle gas mileage with it if the vehicle is made after 1982.  But overall, we can see that adding <code class="highlighter-rouge">model_year</code> as a categorical variable gives us a better regression model.</p>

<p>When we add <code class="highlighter-rouge">model_year</code> to the regression model, the coefficient of <code class="highlighter-rouge">horsepower</code> is no longer significant.  This could indicate that there is <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a> between variables, or that horsepower could be explained by other independent variables.  This is a very important concept in linear regression that deserves careful investigation to tease out.</p>

<blockquote class="challenge">
  <h2 id="challenge--determining-impact-of-variables">Challenge:  Determining Impact of Variables</h2>

  <p>Drop horsepower from the regression model, including only <code class="highlighter-rouge">weight</code>, <code class="highlighter-rouge">origin</code> and <code class="highlighter-rouge">model_year</code>.  What is the impact of this change on the model?  Why?</p>

</blockquote>


<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Choose independent variables carefully.</p>
</li>
    
  </ul>
</blockquote>

</article>

<div class="row">
  <div class="col-xs-1">
    <h3 class="text-left">
      
      <a href="../09-data_prep/"><span class="glyphicon glyphicon-menu-left" aria-hidden="true"></span><span class="sr-only">previous episode</span></a>
      
    </h3>
  </div>
  <div class="col-xs-10">
    
  </div>
  <div class="col-xs-1">
    <h3 class="text-right">
      
      <a href="../11-crisp/"><span class="glyphicon glyphicon-menu-right" aria-hidden="true"></span><span class="sr-only">next episode</span></a>
      
    </h3>
  </div>
</div>


      
      
<footer>
  <div class="row">
    <div class="col-md-6" align="left">
      <h4>
	Copyright &copy; 2016–2020
	
	<a href="https://software-carpentry.org">Software Carpentry Foundation</a>
	
      </h4>
    </div>
    <div class="col-md-6" align="right">
      <h4>
	
	<a href="/edit/gh-pages/_episodes/10-linear_regression.md">Edit on GitHub</a>
	
	/
	<a href="/blob/gh-pages/CONTRIBUTING.md">Contributing</a>
	/
	<a href="/">Source</a>
	/
	<a href="/blob/gh-pages/CITATION">Cite</a>
	/
	<a href="lessons@software-carpentry.org">Contact</a>
      </h4>
    </div>
  </div>
</footer>

      
    </div>
    
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/bootstrap.min.js"></script>
<script src="../assets/js/lesson.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-37305346-2', 'auto');
  ga('send', 'pageview');
</script>

  </body>
</html>
